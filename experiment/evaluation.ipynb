{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e792e5b",
   "metadata": {},
   "source": [
    "# Notebook for running evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72085ccb",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# from experiment.evaluations import evaluate_dispatching #, evaluate_llm_only, evaluate_baseline_rag, evaluate_reranker_rag, evaluate_dense_rag, evaluate_dense_reranker_rag\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from db import pool \n",
    "from typing import List, Dict, Any\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "PROJECT_DIR = os.path.join(ROOT_DIR, \"master-thesis-project\")\n",
    "EXPERIMENT_DIR = os.path.join(PROJECT_DIR, \"experiment\")\n",
    "DATASET_DIR = os.path.join(EXPERIMENT_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb003a7",
   "metadata": {},
   "source": [
    "Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e596d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispatching = await evaluate_dispatching()\n",
    "# llm_only = await evaluate_llm_only()\n",
    "# baseline_rag = await evaluate_baseline_rag()\n",
    "# reranker_rag = await evaluate_reranker_rag()\n",
    "# dense_rag = await evaluate_dense_rag()\n",
    "# dense_reranker_rag = await evaluate_dense_reranker_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798f66a",
   "metadata": {},
   "source": [
    "Read evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42a1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispatching\n",
    "dispatching = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_dispatching.csv\"))\n",
    "\n",
    "# RAG\n",
    "llm_only = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_llm_only.csv\"))\n",
    "baseline_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_baseline_rag.csv\"))\n",
    "reranker_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_reranker_rag.csv\"))\n",
    "dense_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_dense_rag.csv\"))\n",
    "dense_reranker_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_dense_reranker_rag.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb1c16",
   "metadata": {},
   "source": [
    "Dispatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0713b38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatching accuracy: 0.8087\n",
      "Average response time: 7.4747 seconds\n",
      "Dispatching accuracy (excluding InputError): 0.8532\n",
      "Average response time (excluding InputError): 7.4747 seconds\n",
      "                   total  correct  errors  accuracy\n",
      "expected                                           \n",
      "Calculation Agent     59       59       0       1.0\n",
      "Dreamplan Agent       80       32      48       0.4\n",
      "Finance Agent        188      188       0       1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>correct</th>\n",
       "      <th>errors</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expected</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Calculation Agent</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dreamplan Agent</th>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>48</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finance Agent</th>\n",
       "      <td>206</td>\n",
       "      <td>188</td>\n",
       "      <td>18</td>\n",
       "      <td>0.912621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   total  correct  errors  accuracy\n",
       "expected                                           \n",
       "Calculation Agent     59       59       0  1.000000\n",
       "Dreamplan Agent       80       32      48  0.400000\n",
       "Finance Agent        206      188      18  0.912621"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming dispatching DataFrame has columns: 'expected', 'result', 'time'\n",
    "if \"expected\" in dispatching.columns and \"result\" in dispatching.columns and \"time\" in dispatching.columns:\n",
    "    dispatching[\"correct\"] = dispatching[\"expected\"] == dispatching[\"result\"]\n",
    "    accuracy = dispatching[\"correct\"].mean()\n",
    "    avg_time = dispatching[\"time\"].mean()\n",
    "    print(f\"Dispatching accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Average response time: {avg_time:.4f} seconds\")\n",
    "else:\n",
    "    print(\"Required columns ('expected', 'result', 'time') not found in dispatching DataFrame.\")\n",
    "\n",
    "# Filter out rows where result is \"InputError\" and recalculate summary\n",
    "if \"expected\" in dispatching.columns and \"result\" in dispatching.columns and \"time\" in dispatching.columns:\n",
    "    filtered_dispatching = dispatching[dispatching[\"result\"] != \"InputError\"].copy()\n",
    "    filtered_dispatching[\"correct\"] = filtered_dispatching[\"expected\"] == filtered_dispatching[\"result\"]\n",
    "    accuracy = filtered_dispatching[\"correct\"].mean()\n",
    "    avg_time = filtered_dispatching[\"time\"].mean()\n",
    "    print(f\"Dispatching accuracy (excluding InputError): {accuracy:.4f}\")\n",
    "    print(f\"Average response time (excluding InputError): {avg_time:.4f} seconds\")\n",
    "\n",
    "    # Group by 'expected' and count correct and incorrect predictions for filtered data\n",
    "    filtered_summary = filtered_dispatching.groupby('expected').agg(\n",
    "        total=('expected', 'count'),\n",
    "        correct=('correct', 'sum'),\n",
    "        errors=('correct', lambda x: (~x).sum())\n",
    "    )\n",
    "    filtered_summary['accuracy'] = filtered_summary['correct'] / filtered_summary['total']\n",
    "    print(filtered_summary)\n",
    "else:\n",
    "    print(\"Required columns ('expected', 'result', 'time') not found in dispatching DataFrame.\")\n",
    "\n",
    "# Group by 'expected' and count correct and incorrect predictions\n",
    "dispatching_summary = dispatching.groupby('expected').agg(\n",
    "    total=('expected', 'count'),\n",
    "    correct=('correct', 'sum'),\n",
    "    errors=('correct', lambda x: (~x).sum())\n",
    ")\n",
    "dispatching_summary['accuracy'] = dispatching_summary['correct'] / dispatching_summary['total']\n",
    "dispatching_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80e73d",
   "metadata": {},
   "source": [
    "RAG response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db7bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions_dense_reranker_rag.jsonl': 2.6997397374180916,\n",
       " 'questions_llm_only.jsonl': 3.1922133038344893,\n",
       " 'questions_dense_rag.jsonl': 2.4557241386580237,\n",
       " 'questions_reranker_rag.jsonl': 3.042295024232957,\n",
       " 'questions_baseline_rag.jsonl': 2.14312895524849}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonl_files = [\n",
    "    os.path.join(DATASET_DIR, fname)\n",
    "    for fname in os.listdir(DATASET_DIR)\n",
    "    if fname.startswith(\"questions_\") and fname.endswith(\".jsonl\")\n",
    "]\n",
    "avg_response_times = {}\n",
    "for file in jsonl_files:\n",
    "    response_times = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if \"time\" in data:\n",
    "                response_times.append(data[\"time\"])\n",
    "    if response_times:\n",
    "        avg_response_times[os.path.basename(file)] = sum(response_times) / len(response_times)\n",
    "    else:\n",
    "        avg_response_times[os.path.basename(file)] = None\n",
    "\n",
    "avg_response_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb47153",
   "metadata": {},
   "source": [
    "LLM-only vs LLM + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cea506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy_mean</th>\n",
       "      <th>avg_response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM Only</th>\n",
       "      <td>0.770185</td>\n",
       "      <td>3.192213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline RAG</th>\n",
       "      <td>0.857162</td>\n",
       "      <td>2.143129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reranker RAG</th>\n",
       "      <td>0.842157</td>\n",
       "      <td>3.042295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense RAG</th>\n",
       "      <td>0.852620</td>\n",
       "      <td>2.455724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense Reranker RAG</th>\n",
       "      <td>0.814216</td>\n",
       "      <td>2.699740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    answer_relevancy_mean  avg_response_time\n",
       "LLM Only                         0.770185           3.192213\n",
       "Baseline RAG                     0.857162           2.143129\n",
       "Reranker RAG                     0.842157           3.042295\n",
       "Dense RAG                        0.852620           2.455724\n",
       "Dense Reranker RAG               0.814216           2.699740"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = {\n",
    "    \"LLM Only\": llm_only,\n",
    "    \"Baseline RAG\": baseline_rag,\n",
    "    \"Reranker RAG\": reranker_rag,\n",
    "    \"Dense RAG\": dense_rag,\n",
    "    \"Dense Reranker RAG\": dense_reranker_rag,\n",
    "}\n",
    "\n",
    "# Compute mean and std for answer_relevancy for all methods,\n",
    "# and for other columns for RAG methods (Baseline, Reranker, Dense)\n",
    "columns_to_compare = [\"answer_relevancy\", \"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\"]\n",
    "\n",
    "# Compare answer_relevancy for all methods\n",
    "answer_relevancy_means = {method: df[\"answer_relevancy\"].mean() for method, df in methods.items()}\n",
    "answer_relevancy_df = pd.DataFrame.from_dict(answer_relevancy_means, orient=\"index\", columns=[\"answer_relevancy_mean\"])\n",
    "\n",
    "# Compare other metrics for RAG methods only\n",
    "rag_methods = {k: v for k, v in methods.items() if k != \"LLM Only\"}\n",
    "other_metrics = [\"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\"]\n",
    "other_metrics_means = {\n",
    "    method: {metric: df[metric].mean() for metric in other_metrics}\n",
    "    for method, df in rag_methods.items()\n",
    "}\n",
    "other_metrics_df = pd.DataFrame.from_dict(other_metrics_means, orient=\"index\")\n",
    "\n",
    "# Map method names to their corresponding avg response time file\n",
    "method_to_file = {\n",
    "    \"LLM Only\": \"questions_llm_only.jsonl\",\n",
    "    \"Baseline RAG\": \"questions_baseline_rag.jsonl\",\n",
    "    \"Reranker RAG\": \"questions_reranker_rag.jsonl\",\n",
    "    \"Dense RAG\": \"questions_dense_rag.jsonl\",\n",
    "    \"Dense Reranker RAG\": \"questions_dense_reranker_rag.jsonl\",\n",
    "}\n",
    "\n",
    "\n",
    "# Add avg response time to answer_relevancy_df\n",
    "answer_relevancy_df[\"avg_response_time\"] = [\n",
    "    avg_response_times.get(method_to_file[m], None) for m in answer_relevancy_df.index\n",
    "]\n",
    "\n",
    "answer_relevancy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8774c",
   "metadata": {},
   "source": [
    "RAG methods comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bc7f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>nv_context_relevance</th>\n",
       "      <th>answer_relevancy_mean</th>\n",
       "      <th>avg_response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline RAG</th>\n",
       "      <td>0.7575</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>**0.8572**</td>\n",
       "      <td>**2.1431**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reranker RAG</th>\n",
       "      <td>0.7410</td>\n",
       "      <td>0.9142</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.8422</td>\n",
       "      <td>3.0423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense RAG</th>\n",
       "      <td>0.7673</td>\n",
       "      <td>**0.9308**</td>\n",
       "      <td>**0.6262**</td>\n",
       "      <td>0.8526</td>\n",
       "      <td>2.4557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense Reranker RAG</th>\n",
       "      <td>**0.7686**</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>0.6238</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>2.6997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   llm_context_precision_without_reference faithfulness  \\\n",
       "Baseline RAG                                        0.7575       0.9259   \n",
       "Reranker RAG                                        0.7410       0.9142   \n",
       "Dense RAG                                           0.7673   **0.9308**   \n",
       "Dense Reranker RAG                              **0.7686**       0.9198   \n",
       "\n",
       "                   nv_context_relevance answer_relevancy_mean  \\\n",
       "Baseline RAG                     0.5206            **0.8572**   \n",
       "Reranker RAG                     0.5121                0.8422   \n",
       "Dense RAG                    **0.6262**                0.8526   \n",
       "Dense Reranker RAG               0.6238                0.8142   \n",
       "\n",
       "                   avg_response_time  \n",
       "Baseline RAG              **2.1431**  \n",
       "Reranker RAG                  3.0423  \n",
       "Dense RAG                     2.4557  \n",
       "Dense Reranker RAG            2.6997  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlighted = other_metrics_df.copy()\n",
    "for col in highlighted.columns:\n",
    "    max_val = highlighted[col].max()\n",
    "    highlighted[col] = highlighted[col].apply(\n",
    "        lambda x: f\"**{x:.4f}**\" if np.isclose(x, max_val) else f\"{x:.4f}\"\n",
    "    )\n",
    "\n",
    "# Add and highlight answer_relevancy_mean\n",
    "highlighted[\"answer_relevancy_mean\"] = answer_relevancy_df.loc[highlighted.index, \"answer_relevancy_mean\"]\n",
    "max_relevancy = highlighted[\"answer_relevancy_mean\"].max()\n",
    "highlighted[\"answer_relevancy_mean\"] = highlighted[\"answer_relevancy_mean\"].apply(\n",
    "    lambda x: f\"**{x:.4f}**\" if np.isclose(x, max_relevancy) else f\"{x:.4f}\"\n",
    ")\n",
    "\n",
    "# Add and highlight avg_response_time (lowest is best)\n",
    "highlighted[\"avg_response_time\"] = [\n",
    "    avg_response_times.get(method_to_file[m], None) for m in highlighted.index\n",
    "]\n",
    "min_time = highlighted[\"avg_response_time\"].min()\n",
    "highlighted[\"avg_response_time\"] = highlighted[\"avg_response_time\"].apply(\n",
    "    lambda x: f\"**{x:.4f}**\" if np.isclose(x, min_time) else f\"{x:.4f}\"\n",
    ")\n",
    "\n",
    "highlighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b30a3",
   "metadata": {},
   "source": [
    "Statistical significance (including LLM-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac685c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method_1</th>\n",
       "      <th>method_2</th>\n",
       "      <th>wilcoxon_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM Only</td>\n",
       "      <td>Baseline RAG</td>\n",
       "      <td>1.088308e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline RAG</td>\n",
       "      <td>Dense RAG</td>\n",
       "      <td>1.630178e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       method_1      method_2    wilcoxon_p\n",
       "0      LLM Only  Baseline RAG  1.088308e-07\n",
       "1  Baseline RAG     Dense RAG  1.630178e-01"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "# LLM Only vs Baseline RAG\n",
    "arr1 = methods[\"LLM Only\"][\"answer_relevancy\"]\n",
    "arr2 = methods[\"Baseline RAG\"][\"answer_relevancy\"]\n",
    "w_stat, w_p = wilcoxon(arr1, arr2)\n",
    "results.append({\n",
    "    \"method_1\": \"LLM Only\",\n",
    "    \"method_2\": \"Baseline RAG\",\n",
    "    \"wilcoxon_p\": w_p\n",
    "})\n",
    "\n",
    "# Baseline RAG vs Dense RAG\n",
    "arr1 = methods[\"Baseline RAG\"][\"answer_relevancy\"]\n",
    "arr2 = methods[\"Dense RAG\"][\"answer_relevancy\"]\n",
    "w_stat, w_p = wilcoxon(arr1, arr2)\n",
    "results.append({\n",
    "    \"method_1\": \"Baseline RAG\",\n",
    "    \"method_2\": \"Dense RAG\",\n",
    "    \"wilcoxon_p\": w_p\n",
    "})\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b2676",
   "metadata": {},
   "source": [
    "Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c7e3593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparison</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense vs Baseline</td>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.838382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline vs Reranker</td>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.244506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense vs Dense+Reranker</td>\n",
       "      <td>context precision</td>\n",
       "      <td>0.011699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dense vs Dense+Reranker</td>\n",
       "      <td>context relevance</td>\n",
       "      <td>0.625070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dense vs Baseline+Reranker</td>\n",
       "      <td>context relevance</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   comparison             metric   p-value\n",
       "0           Dense vs Baseline       faithfulness  0.838382\n",
       "1        Baseline vs Reranker       faithfulness  0.244506\n",
       "2     Dense vs Dense+Reranker  context precision  0.011699\n",
       "3     Dense vs Dense+Reranker  context relevance  0.625070\n",
       "4  Dense vs Baseline+Reranker  context relevance  0.000612"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_baseline = baseline_rag[\"faithfulness\"]\n",
    "faithfulness_dense = dense_rag[\"faithfulness\"]\n",
    "mask = ~faithfulness_baseline.isna() & ~faithfulness_dense.isna()\n",
    "_, faithfulness_p = wilcoxon(\n",
    "    faithfulness_baseline[mask], faithfulness_dense[mask]\n",
    ")\n",
    "faithfulness_reranker = reranker_rag[\"faithfulness\"]\n",
    "mask_reranker = ~faithfulness_baseline.isna() & ~faithfulness_reranker.isna()\n",
    "_, faithfulness_reranker_p = wilcoxon(\n",
    "    faithfulness_baseline[mask_reranker], faithfulness_reranker[mask_reranker]\n",
    ")\n",
    "_, precision_p = wilcoxon(\n",
    "    dense_rag[\"llm_context_precision_without_reference\"],\n",
    "    dense_reranker_rag[\"llm_context_precision_without_reference\"]\n",
    ")\n",
    "_, relevance_p = wilcoxon(\n",
    "    dense_rag[\"nv_context_relevance\"], dense_reranker_rag[\"nv_context_relevance\"]\n",
    ")\n",
    "_, relevance_dense_vs_reranker_p = wilcoxon(\n",
    "    dense_rag[\"nv_context_relevance\"], reranker_rag[\"nv_context_relevance\"]\n",
    ")\n",
    "\n",
    "\n",
    "comparisons = [\n",
    "    (\"Dense vs Baseline\", \"faithfulness\", faithfulness_p),\n",
    "    (\"Baseline vs Reranker\", \"faithfulness\", faithfulness_reranker_p),\n",
    "    (\"Dense vs Dense+Reranker\", \"context precision\", precision_p),\n",
    "    (\"Dense vs Dense+Reranker\", \"context relevance\", relevance_p),\n",
    "    (\"Dense vs Baseline+Reranker\", \"context relevance\", relevance_dense_vs_reranker_p),\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparisons, columns=[\"comparison\", \"metric\", \"p-value\"])\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb545e",
   "metadata": {},
   "source": [
    "System Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_feedbacks_with_stats() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return feedbacks aggregated per session:\n",
    "    - number of feedbacks\n",
    "    - average correctness, relevance, clarity, satisfaction\n",
    "    - number of replies\n",
    "    - average response time\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            f.session_id,\n",
    "            COUNT(f.id) AS num_feedbacks,\n",
    "            AVG(f.correctness) AS avg_correctness,\n",
    "            AVG(f.relevance) AS avg_relevance,\n",
    "            AVG(f.clarity) AS avg_clarity,\n",
    "            AVG(f.satisfaction) AS avg_satisfaction,\n",
    "            COUNT(r.id) AS num_replies,\n",
    "            AVG(r.response_time) AS avg_response_time,\n",
    "            MIN(f.timestamp) AS first_feedback,\n",
    "            MAX(f.timestamp) AS last_feedback,\n",
    "            ARRAY_AGG(f.comments) AS comments\n",
    "        FROM feedbacks f\n",
    "        LEFT JOIN replies r ON f.session_id = r.session_id\n",
    "        GROUP BY f.session_id\n",
    "        ORDER BY last_feedback DESC\n",
    "    \"\"\"\n",
    "    async with pool.acquire() as conn:\n",
    "        rows = await conn.fetch(query)\n",
    "\n",
    "        # Make it cleaner for display in notebooks\n",
    "        result = []\n",
    "        for row in rows:\n",
    "            result.append({\n",
    "                \"session_id\": row[\"session_id\"],\n",
    "                \"num_feedbacks\": row[\"num_feedbacks\"],\n",
    "                \"avg_scores\": {\n",
    "                    \"correctness\": float(row[\"avg_correctness\"]) if row[\"avg_correctness\"] is not None else None,\n",
    "                    \"relevance\": float(row[\"avg_relevance\"]) if row[\"avg_relevance\"] is not None else None,\n",
    "                    \"clarity\": float(row[\"avg_clarity\"]) if row[\"avg_clarity\"] is not None else None,\n",
    "                    \"satisfaction\": float(row[\"avg_satisfaction\"]) if row[\"avg_satisfaction\"] is not None else None,\n",
    "                },\n",
    "                \"num_replies\": row[\"num_replies\"],\n",
    "                \"avg_response_time\": float(row[\"avg_response_time\"]) if row[\"avg_response_time\"] is not None else None,\n",
    "                \"first_feedback\": row[\"first_feedback\"],\n",
    "                \"last_feedback\": row[\"last_feedback\"],\n",
    "                \"comments\": row[\"comments\"] if row[\"comments\"] else [],\n",
    "            })\n",
    "        return result\n",
    "    \n",
    "feedbacks = await get_feedbacks_with_stats()\n",
    "feedbacks_df = pd.DataFrame(feedbacks)\n",
    "start_time = pd.Timestamp(\"2025-08-18 13:22:00\")\n",
    "\n",
    "filtered_df = feedbacks_df[feedbacks_df[\"first_feedback\"] >= start_time].copy()\n",
    "\n",
    "scores_df = filtered_df[\"avg_scores\"].apply(pd.Series)\n",
    "filtered_df = pd.concat([filtered_df.drop(columns=[\"avg_scores\"]), scores_df], axis=1).drop(columns=[\"session_id\", \"first_feedback\", \"last_feedback\"])\n",
    "filtered_df[\"avg_response_time\"] = round(filtered_df[\"avg_response_time\"] / 1000, 2)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e17b3545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores Statistics:\n",
      "       correctness  relevance    clarity  satisfaction\n",
      "count    41.000000  41.000000  41.000000     41.000000\n",
      "mean      5.048780   5.365854   5.609756      4.878049\n",
      "std       2.682827   2.233339   2.245863      2.293416\n",
      "min       0.000000   0.000000   0.000000      0.000000\n",
      "25%       3.000000   4.000000   4.000000      4.000000\n",
      "50%       5.000000   5.000000   6.000000      5.000000\n",
      "75%       7.000000   7.000000   7.000000      7.000000\n",
      "max       9.000000   9.000000   9.000000      9.000000\n",
      "\n",
      "Aggregated Unique Comments:\n",
      "\n",
      "The bot responded promptly and provided clear and relevant answers to my questions about taxes and general financial topics. However, the investment recommendations seemed unrealistic, particularly the suggested monthly savings and pension depot amounts, which were far higher than typical financial planning guidelines. The explanation of the recommendation was detailed and helpful. Overall, the chat interface was easy to use, but recommendations need calibration to reflect reasonable financial advice. - Testing and Feedback given by Open AI Agent.  \n",
      "\n",
      "It seems that the AI does not understand my questions, so I might be not asking properly the right questions. It is good with the calculation but it does not pick up whenever it asks me a question suggesting new explanations or path, and I answer \"yes\". It doesn't pick it up from there to continue the conversation.\n",
      "But overall, it does help to explain the calculation and the results, and the data inputs it needs to calculate, it guides well. \n",
      "\n",
      "Once I repeated the context it was better but it didn't like simple answers. As you pointed out :) \n",
      "\n",
      "I do have to held the chatbots hand quite a lot. Especially with triggering the calculation to begin with. The responses are not super relevant yet, but I think it is overall during pretty well \n",
      "\n",
      "It doesn't show me the calculation \n",
      "\n",
      "I asked it to show me the calculation, and it showed me a list of calcualtion it could do. I then asked for \"Overall financial plan comparison\", which was mentioned, and then it said this: \n",
      "\"This information is not in my knowledge, sorry. For a comprehensive comparison of overall financial plans in Denmark, including savings, investment strategies, and retirement planning, you might need to consult specialized financial advisors or detailed financial planning resources specific to Denmark.\" \n",
      "\n",
      "I never added liquid savings yet it told me I would have a shortfall and then didn't ask me what my liquid savings were  \n",
      "\n",
      "The prompt has to be rather specific to get the calculation to display \n",
      "\n",
      "I had to massage it a few times to get the calculation going. \"Show me the calculation output and explain it to me\" works pretty well.\n",
      "\n",
      "I asked it to explain a value from the recommendation \"What is the household wealth at pension describing?\" and it was assumed to be a generic question. Then I asked it again more specifically and it replied with an explanation of the number in the calculation result.\n",
      "\n",
      "It does seem like it needs some work with correctly specifying the underlying agent to use - but typically gets it with some guidance. \n",
      "\n",
      "The second answer didn't answer my question but it gave a fine overall answer regarding what you need to get a house - if that makes sense??? \n",
      "\n",
      "Started by telling it my age, but after submitting that immediately received \"The calculation is complete. If you have additional financial data to include or need further assistance, please let me know!\"\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Struggled with context and  \n",
      "\n",
      "Good - Very thorough explanation of the calculation.  \n",
      "\n",
      "Seems to do the calculation but doesn't share the output \n",
      "\n",
      "i worked okay, it does not give me specific information about value and also got it wrong that im renting my apartment\n",
      " \n",
      "\n",
      "i worked and gave me some information, but could not help me further even though it said it could. \n",
      "\n",
      "It said it could \"help you develop a plan to improve your savings and retirement readiness\", but when I asked, it didn't do it. Tried to prompt again and it told me could go to live in Denmark, and Skat  \n",
      "\n",
      "I answered the first question right but I don't think it understood the second one :-) \n",
      "\n",
      "It even got the sources from the RAG when I asked about it. \n",
      "\n",
      "I asked a variety of tax - salary related questions and was satisfied with the additional knowledge it provided. Very good! :) \n",
      "\n",
      "Not sure why assistant is asking me age and salary again, when i specifically provided them in the first message.\n",
      "\n",
      "- mandeep \n",
      "\n",
      "Unsure of the best prompt structure to generate a recommendation that show results. \n",
      "\n",
      "I provided my age (52) and my salary (45000) but still got \"Your question is not pertinent to the current context. Please rephrase your question.\" and not until I provided a lot of context did the model pick up on age and salary \n",
      "\n",
      "The response is a long the same lines and seems to understand the goal with my prompt, however it makes up this calculation on no data - I have only provided my salary, nothing about my current savings\n",
      " \n",
      "\n",
      "I didn't say how much I actually should be putting a side or anything \n",
      "\n",
      "It would be nice if assistant asked me more questions before calculating on incomplete data.  \n",
      "\n",
      "- mandeep\n",
      "\n",
      " \n",
      "\n",
      "It would be nice if the assistant prompted me for more data in order to be more precise with the calculation rather than immediately prompting me to contact an advisor \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print statistics for avg scores\n",
    "stats = filtered_df[[\"correctness\", \"relevance\", \"clarity\", \"satisfaction\"]].describe()\n",
    "print(\"Average Scores Statistics:\")\n",
    "print(stats)\n",
    "\n",
    "# Aggregate all comments into one list and print\n",
    "all_comments = []\n",
    "for comments in filtered_df[\"comments\"]:\n",
    "    all_comments.extend([c for c in comments if c and c.strip()])\n",
    "    # Print only unique comments\n",
    "unique_comments = list(dict.fromkeys(all_comments))\n",
    "\n",
    "print(\"\\nAggregated Unique Comments:\\n\")\n",
    "for comment in unique_comments:\n",
    "    print(comment, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
