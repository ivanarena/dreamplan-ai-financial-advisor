{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e792e5b",
   "metadata": {},
   "source": [
    "# Notebook for running evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72085ccb",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f6192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# from experiment.evaluations import evaluate_dispatching, evaluate_llm_only, evaluate_baseline_rag, evaluate_reranker_rag, evaluate_dense_rag\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "PROJECT_DIR = os.path.join(ROOT_DIR, \"master-thesis-project\")\n",
    "EXPERIMENT_DIR = os.path.join(PROJECT_DIR, \"experiment\")\n",
    "DATASET_DIR = os.path.join(EXPERIMENT_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb003a7",
   "metadata": {},
   "source": [
    "Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatching = await evaluate_dispatching()\n",
    "llm_only = await evaluate_llm_only()\n",
    "baseline_rag = await evaluate_baseline_rag()\n",
    "reranker_rag = await evaluate_reranker_rag()\n",
    "dense_rag = await evaluate_dense_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798f66a",
   "metadata": {},
   "source": [
    "Read evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42a1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_only = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_llm_only.csv\"))\n",
    "baseline_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_baseline_rag.csv\"))\n",
    "reranker_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_reranker_rag.csv\"))\n",
    "dense_rag = pd.read_csv(os.path.join(DATASET_DIR, \"evaluation_dense_rag.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cea506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              answer_relevancy_mean\n",
       " LLM Only                   0.801779\n",
       " Baseline RAG               0.861880\n",
       " Reranker RAG               0.842789\n",
       " Dense RAG                  0.814879,\n",
       "               llm_context_precision_without_reference  faithfulness  \\\n",
       " Baseline RAG                                 0.760344      0.937853   \n",
       " Reranker RAG                                 0.736470      0.915170   \n",
       " Dense RAG                                    0.768447      0.919765   \n",
       " \n",
       "               nv_context_relevance  \n",
       " Baseline RAG              0.516990  \n",
       " Reranker RAG              0.521845  \n",
       " Dense RAG                 0.626214  )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = {\n",
    "    \"LLM Only\": llm_only,\n",
    "    \"Baseline RAG\": baseline_rag,\n",
    "    \"Reranker RAG\": reranker_rag,\n",
    "    \"Dense RAG\": dense_rag\n",
    "}\n",
    "\n",
    "# Compute mean and std for answer_relevancy for all methods,\n",
    "# and for other columns for RAG methods (Baseline, Reranker, Dense)\n",
    "columns_to_compare = [\"answer_relevancy\", \"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\"]\n",
    "\n",
    "# Compare answer_relevancy for all methods\n",
    "answer_relevancy_means = {method: df[\"answer_relevancy\"].mean() for method, df in methods.items()}\n",
    "answer_relevancy_df = pd.DataFrame.from_dict(answer_relevancy_means, orient=\"index\", columns=[\"answer_relevancy_mean\"])\n",
    "\n",
    "# Compare other metrics for RAG methods only\n",
    "rag_methods = {k: v for k, v in methods.items() if k != \"LLM Only\"}\n",
    "other_metrics = [\"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\"]\n",
    "other_metrics_means = {\n",
    "    method: {metric: df[metric].mean() for metric in other_metrics}\n",
    "    for method, df in rag_methods.items()\n",
    "}\n",
    "other_metrics_df = pd.DataFrame.from_dict(other_metrics_means, orient=\"index\")\n",
    "\n",
    "answer_relevancy_df, other_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32bc7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30015/270250590.py:33: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  overall_means_incl_relevancy = highlighted[[\"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\"]].applymap(lambda x: float(x.replace(\"**\", \"\"))) \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>nv_context_relevance</th>\n",
       "      <th>Best Overall</th>\n",
       "      <th>answer_relevancy_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline RAG</th>\n",
       "      <td>0.7603</td>\n",
       "      <td>**0.9379**</td>\n",
       "      <td>0.5170</td>\n",
       "      <td></td>\n",
       "      <td>**0.8619**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reranker RAG</th>\n",
       "      <td>0.7365</td>\n",
       "      <td>0.9152</td>\n",
       "      <td>0.5218</td>\n",
       "      <td></td>\n",
       "      <td>0.8428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense RAG</th>\n",
       "      <td>**0.7684**</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>**0.6262**</td>\n",
       "      <td>üèÜ</td>\n",
       "      <td>0.8149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             llm_context_precision_without_reference faithfulness  \\\n",
       "Baseline RAG                                  0.7603   **0.9379**   \n",
       "Reranker RAG                                  0.7365       0.9152   \n",
       "Dense RAG                                 **0.7684**       0.9198   \n",
       "\n",
       "             nv_context_relevance Best Overall answer_relevancy_mean  \n",
       "Baseline RAG               0.5170                         **0.8619**  \n",
       "Reranker RAG               0.5218                             0.8428  \n",
       "Dense RAG              **0.6262**            üèÜ                0.8149  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Highlight best in each metric (highest value)\n",
    "highlighted = other_metrics_df.copy()\n",
    "for col in highlighted.columns:\n",
    "    max_val = highlighted[col].max()\n",
    "    highlighted[col] = highlighted[col].apply(lambda x: f\"**{x:.4f}**\" if np.isclose(x, max_val) else f\"{x:.4f}\")\n",
    "\n",
    "# Find best overall (highest mean across metrics)\n",
    "overall_means = other_metrics_df.mean(axis=1)\n",
    "best_overall_method = overall_means.idxmax()\n",
    "\n",
    "# Add a column to indicate best overall\n",
    "highlighted[\"Best Overall\"] = \"\"\n",
    "highlighted.loc[best_overall_method, \"Best Overall\"] = \"üèÜ\"\n",
    "\n",
    "highlighted\n",
    "# Add answer_relevancy_mean to highlighted table and highlight best\n",
    "highlighted[\"answer_relevancy_mean\"] = answer_relevancy_df.loc[highlighted.index, \"answer_relevancy_mean\"].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "max_relevancy = answer_relevancy_df[\"answer_relevancy_mean\"].max()\n",
    "for idx in highlighted.index:\n",
    "    val = answer_relevancy_df.loc[idx, \"answer_relevancy_mean\"]\n",
    "    if np.isclose(val, max_relevancy):\n",
    "        highlighted.at[idx, \"answer_relevancy_mean\"] = f\"**{val:.4f}**\"\n",
    "\n",
    "# Compute overall mean including answer_relevancy_mean\n",
    "metrics_for_overall = [\"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\", \"answer_relevancy_mean\"]\n",
    "\n",
    "# Convert answer_relevancy_mean to float for calculation\n",
    "highlighted[\"answer_relevancy_mean_float\"] = answer_relevancy_df.loc[highlighted.index, \"answer_relevancy_mean\"]\n",
    "\n",
    "overall_means_incl_relevancy = highlighted[[\"llm_context_precision_without_reference\", \"faithfulness\", \"nv_context_relevance\"]].applymap(lambda x: float(x.replace(\"**\", \"\"))) \\\n",
    "    .join(highlighted[\"answer_relevancy_mean_float\"]).mean(axis=1)\n",
    "\n",
    "best_overall_method_incl_relevancy = overall_means_incl_relevancy.idxmax()\n",
    "\n",
    "# Update Best Overall column\n",
    "highlighted[\"Best Overall\"] = \"\"\n",
    "highlighted.loc[best_overall_method_incl_relevancy, \"Best Overall\"] = \"üèÜ\"\n",
    "\n",
    "# Remove helper column\n",
    "highlighted = highlighted.drop(columns=[\"answer_relevancy_mean_float\"])\n",
    "\n",
    "highlighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343b35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
